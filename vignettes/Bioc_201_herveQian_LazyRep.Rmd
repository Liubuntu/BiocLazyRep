---
author:
- name: Hervé Pagès
  affiliation: Fred Hutchinson Cancer Research Center, Seattle, WA
- name: Qian Liu
  affiliation: Roswell Park Comprehensive Cancer Center, Buffalo, NY
- name: Martin Morgan
  affiliation: Roswell Park Comprehensive Cancer Center, Buffalo, NY
vignette: >
  %\VignetteIndexEntry{Lazy Representations of Very Large Genomic Data Resources in R/Bioconductor}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}{inputenc}
date: "Last modified: May 30, 2019; Compiled: `r format(Sys.time(), '%B %d, %Y')`"
output:
    rmarkdown::html_document:
    highlight: pygments
    toc: true
    toc_depth: 3
    fig_width: 5
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
```{r options, include = FALSE}
options(showHeadLines=3)
options(showTailLines=3)
```



# Lazy Representations of Very Large Genomic Data Resources in R/Bioconductor



## About this workshop


### Workshop description

In this workshop, we will learn about _Bioconductor_ data containers that
use lazy data representations and about their application in real data
analysis. We will use some real data examples generated from DNA/RNA-seq,
to demonstrate the representation and comprehension of large scale
(out-of-memory) genomic datasets. The workshop will be mainly instructor-led
live demo with completely working examples. Instructions and notes will be
included.


### Instructors and contact information

- **Hervé Pagès**, Fred Hutchinson Cancer Research Center, Seattle, WA
- **Qian Liu**, Roswell Park Comprehensive Cancer Center, Buffalo, NY
- **Martin Morgan**, Roswell Park Comprehensive Cancer Center, Buffalo, NY


### Pre-requisites

- Basic knowledge of R syntax (`matrix`, `array`, `data.frame`, etc.)
- Some familiarity with manipulation of S4 objects in general
- Some familiarity with `SummarizedExperiment` objects


### Learning objectives

- Introduction to `DelayedArray` objects and related concepts (seed,
  delayed operations, realization, block-processed operations, in-memory
  vs on-disk backends, etc...)

- Use `VariantExperiment` to represent your DNA-seq data.

- Statistical analysis of variant data (VCF) using `VariantExperiment`.

- Create `SQLDataFrame` from in-memory data, text format file, and
  database tables.

- Use `SQLDataFrame` to represent and manipulate orgdb database tables.


### Workshop participation

Students will be using their laptops with internet connection, follow
the instructor to read course materials and run through the working
code chunks.


### Time outline

| Activity | Time |
|----------|------|
| DelayedArray and HDF5Array objects | 30m |
| DelayedArray and HDF5Array hands-on | 10m |
| Specialized DelayedArray backends | 10m |
| VariantExperiment | 20m |
| DelayedDataFrame | 10m |
| SQLDataFrame | 10m |
| Future directions | 5-10m |


###  _R/Bioconductor_ packages

The workshop makes use of the following packages:

```{r setup, message = FALSE}
library(DelayedArray)
library(HDF5Array)
library(pryr)
library(DelayedMatrixStats)
library(SummarizedExperiment)
library(gdsfmt)
library(SeqArray)
library(GDSArray)
library(VCFArray)
library(DelayedDataFrame)
library(SQLDataFrame)
library(VariantAnnotation)
library(org.Hs.eg.db)
library(TxDb.Hsapiens.UCSC.hg38.knownGene)
library(DBI)
#> library(VariantExperiment)
```



## DelayedArray and HDF5Array objects


### Motivation and challenges

R ordinary matrices or arrays are not suitable for big datasets.

For example the 10x Genomics dataset (single cell experiment) has
30,000 rows (genes) and 1.3 million columns (cells). It would
require 136G of memory to load it in an ordinary integer matrix!

We still want to be able to work with this dataset in R. Ideally,
manipulation of the dataset in R should be as easy as with an
ordinary matrix. In particular, we'd like to be able to use things
like `dim`, `dimnames`, `t`, `[`, `is.na`, `==`, `+`, `log`, `cbind`,
`max`, `sum`, `colSums`, etc...


### What is a DelayedArray object?

A DelayedArray object is a data structure that can be used to wrap any
matrix-like or array-like dataset. The dataset will typically be an
_on disk_ dataset but it can also be an _in memory_ dataset. This
wrapping serves 2 purposes: (1) it allows to reduce considerably the
amount of memory needed to perform common matrix or array operations
on the dataset, and (2) it allows operating on the dataset while
treating it as read-only.

The DelayedArray class is an S4 class defined in the _DelayedArray_
package. A simple way to create a DelayedArray object is to call the
`DelayedArray()` constructor on a matrix-like or array-like object:

```{r, DelayedArray}
library(DelayedArray)

m0 <- matrix(1:30, nrow=5)
M <- DelayedArray(m0)
M

a0 <- array(runif(15000000), dim=c(10000, 300, 5))
A <- DelayedArray(a0)
A
```

See `?DelayedArray` for more information about the DelayedArray constructor.

The matrix-like or array-like object passed to the constructor is called
the _seed_ of the DelayedArray object.

The seed can be an _in-memory_ dataset (e.g. an ordinary matrix or array,
or a dgCMatrix object from the _Matrix_ package), or an _on-disk_ dataset
(e.g. an HDF5 dataset).

Here is an example of a DelayedArray object where the seed is an HDF5 dataset:

```{r, HDF5Array}
library(HDF5Array)
A0 <- as(a0, "HDF5Array")  # write 'a0' to an HDF5 file
A0
```

An HDF5Array is a particular kind of DelayedArray object:

```{r}
is(A0, "DelayedArray")
```

`A0`'s memory footprint is very small because the data is on disk:

```{r}
library(pryr)  # for object_size()
object_size(A0)
object_size(a0)
```

Think of `A0` as a pointer to the HDF5 dataset. To see where `A0` is pointing
at, look at its seed:

```{r}
seed(A0)  # an HDF5ArraySeed object
```

DelayedArray objects support many common matrix or array operations:

```{r}
A1 <- log(A0 + 1)
A1

sum(A1)
```


### Operations on DelayedArray objects

Operations on a DelayedArray object are either _delayed_ or _block-processed_.

#### Delayed operations

For example the `+ 1` and `log` operations in `A1 <- log(A0 + 1)` are
_delayed_. This means that they are very fast and produce a new DelayedArray
object with the same seed as the input object:

```{r}
seed(A1)

identical(seed(A0), seed(A1))
```

Note that `A1`'s memory footprint is still very small because the dataset
remains on disk:

```{r}
object_size(A1)

showtree(A1)
```

One difference between `A0` and `A1` is that `A1` now carries _delayed
operations_.

Another important difference is that `A1` is no longer an HDF5Array instance.
It is now a DelayedArray instance:

```{r, class_A0_vs_class_A1}
class(A0)

class(A1)
```

This is an important property of _delayed operations_: they will demote
any specialized DelayedArray object to a DelayedArray or DelayedMatrix
instance.

We could keep going:

```{r}
M2 <- t(A1[ , 1:20, 5])
M2
```

The seed is still the same. This only adds more _delayed operations_ to be
applied to it:

```{r}
seed(M2)

object_size(M2)

showtree(M2)
```

An important principle: The seed of a DelayedArray object is **always**
treated as a _read-only_ object so will never be modified by the operations
we perform on the object.

Summary of _delayed operations_ supported on DelayedArray objects:

| Operations | Note |
|------------|------|
| `x[i_1, i_2, ..., i_n]` | _Multi-dimensional_ single bracket subsetting |
| `t`, `aperm` | |
| `rbind`, `cbind` |  |
| `sweep` | |
| Members of the `Arith` group | e.g. `+`, `-`, `*`, etc... |
| Members of the `Compare` group | e.g. `==`, `<=`, `<`, etc... |
| Members of the `Logic` group | `&`, `|` |
| Some members of the `Math` group | e.g. `log`, `floor`, etc... |
| Members of the `Math2` group | `round`, `signif` |
| `is.na`, `is.finite`, `is.infinite`, `is.nan` | |
| `pmax2` and `pmin2` | replacements for `base::pmax` and `base::pmin` |
| `dnorm`, `pnorm`, `qnorm` | The Normal Distribution |
| `dbinom`, `pbinom`, `qbinom` | The Binomial Distribution |
| `dpois`, `ppois`, `qpois` | The Poisson Distribution |
| `dlogis`, `plogis`, `qlogis` | The Logistic Distribution |
| `nchar`, `tolower`, `toupper`, `grepl`, `sub`, `gsub` | |
| `!` | |
| `lengths` | only meaningful when object is of type `list` |

#### Realization

To _realize_ a DelayedArray object (i.e. to trigger execution of the
_delayed operations_ carried by the object) one can call `as.array` on it:

```{r, as.array}
m <- as.array(M2)  # in-memory realization
```

Note that `as.array()` returns an ordinary array which means that the
DelayedArray object is realized _in memory_. This could require too much
memory if the object is big. A big DelayedArray object is preferrably
realized _on disk_ for example by calling `writeHDF5Array` on it (this
function is defined in the _HDF5Array_ package):

```{r, writeHDF5Array}
M3 <- writeHDF5Array(M2)  # on-disk realization
M3
```

Coercion to HDF5Array can also be used to realize the object:

```{r, coercion-to-HDF5Array}
M3 <- as(M2, "HDF5Array")  # equivalent to writeHDF5Array(M2)
```

Other on-disk backends can be supported.

On-disk realization uses a _block processing_ mechanism where the
blocks are realized in memory and written to disk one at a time.
This allows realization of HDF5 datasets that don't fit in memory.
See `?writeHDF5Array` in the _HDF5Array_ package for more information.

#### Block-processed operations

When operations are not _delayed_ they are _block-processed_ i.e.
yhey are implemented via a _block processing_ mechanism that loads
and processes one block at a time. For example:

```{r, mean}
mean(A0)  # same as mean(a0)
```

To see the _block processing_ mechanism in action we make it verbose:

```{r, set_verbose_block_processing}
DelayedArray:::set_verbose_block_processing(TRUE)
mean(A0)
```

The size and other aspects of the blocks can be controlled. See
`?setAutoBlockSize` for more information.

Summary of _block-processed_ operations supported on DelayedArray objects:

| Operations | Note |
|------------|------|
| `x[i]` | _Linear_ single bracket subsetting |
| Members of the `Summary` group | e.g. `max`, `sum`, `all`, etc... |
| `mean` | |
| `anyNA`, `which` | |
| `row/colSums`, `row/colMeans` | Base row/col summarization |
| `row/colMaxs`, `row/colProds`, etc... | Extended row/col summarization |
| `rowsum`, `colsum` | |
| `unique`, `table` | |
| `apply` | |

Note: the _Extended row/col summarization_ functions are defined in the
_matrixStats_ package and the methods for DelayedMatrix objects are
implemented in the
[DelayedMatrixStats package](https://bioconductor.org/packages/DelayedMatrixStats).


### On-disk vs in-memory backend

DelayedArray subclasses can be defined to implement specific
backends. For example HDF5Array is a DelayedArray subclass that implements
a DelayedArray backend for HDF5 datasets. Other backends are available
e.g. GDSArray and VCFArray are DelayedArray subclasses that implement
backends for GDS and VCF files, respectively. (They will be covered in
details later in this workshop).

Note that all the above backends are _on-disk backends_.

The DelayedArray framework also supports _in-memory backends_.
For example RleArray is a DelayedArray subclass that implements a
backend where the array data resides in memory and is in RLE form
to reduce its memory footprint. The RleArray class and its constructor
function are implemented in the _DelayedArray_ package:

```{r, RleArray}
data <- Rle(sample(6L, 500000, replace=TRUE), 8)
data

## `data` gets expanded (array() calls as.vector() on it):
a3 <- array(data, dim=c(50, 20, 4000))

## `data` does NOT get expanded (i.e. it remains in RLE form):
A3 <- RleArray(data, dim=c(50, 20, 4000))
A3

object_size(a3)
object_size(A3)

seed(A3)
```

`A3` being a DelayedArray derivative, all the operations (_delayed_ and
_block-processed_) supported on DelayedArray objects work on it:

```{r, }
is(A3, "DelayedArray")

M4 <- t(log(A3[ , , 1] - 0.5))
M4
```

Note that `M4` is no longer an RleArray instance:

```{r, class_A3_vs_class_M4}
class(A3)

class(M4)
```

`M4` can be realized as an ordinary matrix or as an HDF5Matrix object:

```{r}
a4 <- as.matrix(M4)   # same as as.array(M4)

as(M4, "HDF5Matrix")  # same as as(M4, "HDF5Array")
```


### Implementing a DelayedArray backend

The DelayedArray framework currently supports a small number of on-disk
backends: HDF5 (via the _HDF5Array_ package), GDS (via the _GDSArray_
package), and VCF (via the _VCFArray_ package). This can be extended
to support other on-disk backends. In theory, it should be possible to
implement a DelayedArray backend for any file format that has the
capability to store array data with fast random access.

The process of implementing a DelayedArray backend is documented in the
_Implementing A DelayedArray Backend_ vignette from the _DelayedArray_
package. The lastest version of this document is always available at
https://bioconductor.org/packages/devel/DelayedArray

The DelayedArray framework has been designed to make this process as
simple as possible. In particular the number of methods that the
developer needs to implement to support a particular backend has been
kept to a strict minimum of 3 methods: `dim()`, `dimnames()`, and
`extract_array()`.

`extract_array` is a generic function defined in the _DelayedArray_
package. It is not intended to be used directly by the end user.

Note that supporting `dim()`, `dimnames()`, and `extract_array()`
is the only thing that the seed supplied to the `DelayedArray()`
constructor needs to support so this is called _the seed contract_.

In other words the `DelayedArray()` constructor will work and return
a valid DelayedArray object on any seed that supports _the seed contract_.
Then all the operations (_delayed_ and _block-processed_) supported on
DelayedArray objects will work out-of-the-box on the returned object.

Therefore implementing a new DelayedArray backend is just a matter
of implementing a new type of seed that complies with _the seed contract_.
And because _the seed contract_ contract is simple and minimalist,
this process is actually more straightforward than it might sound.

See the _Implementing A DelayedArray Backend_ vignette from the
_DelayedArray_ package for more information.


### Where to learn more and get help

- Don't miss the _Effectively using the DelayedArray framework to support
  the analysis of large datasets_ workshop by Pete Hickey if you are
  planning to use DelayedArray objects in your package or if you just
  want to learn more about DelayedArray objects.

- The _HDF5Array_ and _DelayedArray_ man pages.

- Ask user-oriented questions on our support site: https://support.bioconductor.org

- Ask developer-oriented questions on the bioc-devel mailing list: https://stat.ethz.ch/mailman/listinfo/bioc-devel

- Report bugs and request features on GitHub: https://github.com/Bioconductor/DelayedArray


### Hands-on

1. Load the `airway` dataset (from the _airway_ package).

2. The dataset is wrapped in a SummarizedExperiment object.
   Get the count data as an ordinary matrix.

3. Turn the matrix of counts into an HDF5Matrix object: first
   using `writeHDF5Array`, then using coercion.

4. When using coercion, where has the data been written on disk?

5. See `?setHDF5DumpFile` for how to control the location of
   "automatic" HDF5 datasets. Try to control the destination of
   the data when coercing.

6. Use `showHDF5DumpLog()` to see all the HDF5 datasets written to
   disk during the current session.

7. Try some operations on the HDF5Matrix object: some delayed ones and
   some non-delayed ones (block-processed).

8. Use `DelayedArray:::set_verbose_block_processing(TRUE)` to see
   block processing in action.

9. Change the block size with `setAutoBlockSize()`.

10. Stick the HDF5Matrix object back in the SummarizedExperiment object.
    The resulting object is an "HDF5-backed SummarizedExperiment object".

11. The HDF5-backed SummarizedExperiment object can be manipulated
    (almost) like an in-memory SummarizedExperiment object.
    Try `[`, `cbind`, `rbind` on it.

12. The _HDF5Array_ package provides `saveHDF5SummarizedExperiment()`
    to save a SummarizedExperiment object (HDF5-backed or not) as
    an HDF5-backed SummarizedExperiment object (see
    `?saveHDF5SummarizedExperiment`). Try it.



## Specialized DelayedArray backends


### GDSArray: A DelayedArray extension for GDS data

#### CoreArray and Genomic Data Structure

[CoreArray][] (C++ library) is designed for portable and scalable
storage of large-scale bioinformatics data, which are usually much
larger than the available random-access memory. The data format of
Genomic Data Structure (GDS) is used to store multiple array-oriented
datasets in a single file. The _Bioconductor_ package [gdsfmt][] has
provided a high-level R interface for [CoreArray][].

The GDS format has been widely used in genetic/genomic research for
high-throughput genotyping or sequencing data.  There are two major S3
classes that extends the basic 'gds.class': 'SNPGDSFileClass' (defined
in [SNPRelate][]) suited for genotyping data (e.g., GWAS), and
`SeqVarGDSClass` (defined in [SeqArray][]) that are designed
specifically for DNA-sequencing data. The file format attribute in
each data class is set as 'SNP_ARRAY' and 'SEQ_ARRAY'. There are rich
functions written based on these data classes for common data
operation and statistical analysis in the R packages of [SNPRelate][]
and [SeqArray][].

More details about GDS can be found in the above mentioned package
vignettes. Additional information about CoreArray and GDS could also
be found [here](https://www.biostat.washington.edu/sites/default/files/modules/GDS_intro.pdf).

[CoreArray]: http://corearray.sourceforge.net/
[gdsfmt]: https://bioconductor.org/packages/gdsfmt
[SNPRelate]: https://bioconductor.org/packages/SNPRelate
[SeqArray]: https://bioconductor.org/packages/SeqArray

The GDS file inside R uses hierarchical structure:

```{r, GDSopen}
file <- seqExampleFileName("gds")
g1 <- gdsfmt::openfn.gds(file)
g1
```

Specific functions in manipulating the GDS file:

Package | Function | Description
--------|----------|--------------
gdsfmt | createfn.gds | Create a gds file
gdsfmt | openfn.gds | open a gds file in R
gdsfmt | add.gdsn | add a gds node
gdsfmt | assign.gdsn | assign values into gds node
gdsfmt | closefn.gds | close a gds instance
SeqArray | seqVCF2GDS | Reformat a VCF file
SeqArray | seqSetFilter | Define a subset of samples or variants
SeqArray | seqGetData | Get data with a defined filter
SeqArray | seqApply | Apply a user-defined function over array margins
SeqArray | seqParallel | Apply a function in parallel

For example, `index.gdsn()` will be used to refer to a gds node, and
`readex.gdsn()` is the function to call for subset reading, by
providing the index node, and the indexes as a list.

```{r, GDSread}
sample.node <- index.gdsn(g1, "sample.id")
readex.gdsn(sample.node, 1:5)
geno.node <- index.gdsn(g1, "genotype/data")
aperm(
    readex.gdsn(geno.node, list(1, 1:5, 1:5)),
    c(3,2,1))
closefn.gds(g1)
```

These functions corresponds to the GDS data structure and works best
 with the indexing strategy and compression methods. But at the same
 time, These usual operations on manipulating files and contents
 require much efforts from users in order to make full use of these
 packages.

#### GDSArray, GDSMatrix, and GDSFile

To fully make use of the GDS format, and at the same time, to provide
users a familiar interface with common operations and methods within
the _R/Bioconductor_ ecosystem, We have developed the package of
[GDSArray][] to represent GDS files as DelayedArray instances. It
obeys the seed contract of [DelayedArray][] by supporting `dim`,
`dimnames` methods, and inherits array-like operations and methods
from DelayedArray, e.g., the subsetting method of `[`.

The `GDSArray()` constructor takes arguments for the GDS file path and
the GDS node name, with one GDSArray object only representing one
GDS node. The `GDSArray()` built from GDS file always returns with
rows being "features" (genes / variants / snps) and the columns being
"samples", which is consistent with the assay data inside
SummarizedExperiment and easy to fit into a SummarizedExperiment
object.

```{r, GDSArray}
file <- SeqArray::seqExampleFileName("gds")
GDSArray(file, "genotype/data")
```
Note that a GDSMatrix will be returned by default for 2-dimensional
GDSArray.

```{r, GDSMatrix}
GDSArray(file, "phase/data")
```

The 'GDSFile' is a light-weight class to represent GDS files. It has
the `$` completion method to complete any possible gds nodes. It could
be used as a convenient GDSArray constructor if the slot of
`current_path` in GDSFile object represents a valid gds node.
Otherwise, it will return the GDSFile object with an updated
`current_path`.

```{r, GDSFile}
gf <- GDSFile(file)
gf$annotation$info
gf$annotation$info$AC
```
Try typing in `gf$ann` and pressing tab key for the
auto-completion. (Changes made in GDSArray, wait for new build
tomorrow June 7th.)

`gdsfile()` function returns the file path of the corresponding GDS
  file.
```{r, gdsfileAccessor}
gdsfile(gf)
```

#### methods

GDSArray instances can be subset, following the usual _R_
conventions, with numeric or logical vectors; logical vectors are
recycled to the appropriate length.

```{r, GDSArray methods}
ga <- GDSArray(file, "genotype/data")
ga[1:3, 10:15, ]
ga[c(TRUE, FALSE), , ]
```

some numeric calculation:
```{r, GDSArray numeric}
dp <- GDSArray(file, "annotation/format/DP/data")
log(dp)
dp[rowMeans(dp) < 60, ]
```

**Summary:**

GDSArray represents GDS file as objects derived from DelayedArray
class. It converts a GDS node in the file to a DelayedArray derived
data structure. The rich common methods and data operations defined on
GDSArray makes it more R-user-friendly than working with the GDS file
directly.

The array data from GDS files are always returned with the first
dimension being "variants/snps" and the second dimension being
"samples". This feature is consistent with the assay data saved in
SummarizedExperiment, and makes the [GDSArray][] package easily
interoperable with other established _Bioconductor_ data
infrastructure and methods.



### VCFArray: A DelayedArray extension for VCF data


#### Introduction

The Variant Call Format (VCF) was designed to store gene sequence
variations for the large-scale genotyping and DNA sequencing projects,
(e.g., the 1000 Genomes Project). With the increasing sample size and
sequencing depth, the VCF file also gets much larger than ever and
hardly fit the memory limit of _R_.

[VCFArray][] is a _Bioconductor_ package that represents VCF data
entries as objects derived from the [DelayedArray][] data
structure. The backend VCF file could either be saved on-disk locally
or remote as online resources. Supported data entries include the
fixed data fields (REF, ALT, QUAL, FILTER), information field (e.g.,
AA, AF...), and the individual format field (e.g., GT, DP, etc.). Full
list of supported data entries could be retrieved by `vcfFields()`
method (see details below).

The array data generated from fixed/information fields are
one-dimensional VCFArray, with the dimension being the length of the
variants. The array data generated from individual FORMAT field are
always returned with the first dimension being 'variants' and the
second dimension being 'samples'. This feature is consistent with the
assay data saved in SummarizedExperiment, and makes the [VCFArray][]
package easily interoperable with other established _Bioconductor_
data infrastructure.

[VCFArray]: https://bioconductor.org/packages/VCFArray

#### vcfFields()

The `vcfFields()` method takes the character string (VCF file path),
'VcfFile' object or 'RangedVcfStack' object as input, and returns a
CharacterList with all available VCF fields within specific
categories. Users should consult the 'fixed', 'info' and 'geno'
category for available data entries that could be converted into
VCFArray instances. The data entry names can be used as input for
the 'name' argument in VCFArray constructor.

```{r avail, message=FALSE}
args(VCFArray)
fl <- system.file("extdata", "chr22.vcf.gz", package = "VariantAnnotation")
library(VariantAnnotation)
vcfFields(fl)
```

#### VCFArray, VCFMatrix and vcffile()

We can construct the VCFArray object from the same input as
`vcfFields()` methods (the character string for VCF file path,
'VcfFile' object or 'RangedVcfStack' object).

With a simplest example, we can construct a VCFArray object for the
'GT' data entry in the provided VCF file with arguments of 'file' and
'name' only.

```{r VCFArray constructor}
## character string
VCFArray(file = fl, name = "GT")

## "VcfFile"
vcf <- VariantAnnotation::VcfFile(fl)
VCFArray(file = vcf, name = "DS")

## "RangedVcfStack"
extdata <- system.file(package = "GenomicFiles", "extdata")
files <- dir(extdata, pattern="^CEUtrio.*bgz$", full=TRUE)[1:2]
names(files) <- sub(".*_([0-9XY]+).*", "\\1", basename(files))
seqinfo <- as(readRDS(file.path(extdata, "seqinfo.rds")), "Seqinfo")
stack <- GenomicFiles::VcfStack(files, seqinfo)
gr <- as(GenomicFiles::seqinfo(stack)[rownames(stack)], "GRanges")
## RangedVcfStack
rgstack <- GenomicFiles::RangedVcfStack(stack, rowRanges = gr)
rgstack

vcfFields(rgstack)$geno
VCFArray(rgstack, name = "SB")
```

the backend VCF file could also be remote files. Here we included an
example of representing VCF file of chromosome 22 from the 1000
Genomes Project (Phase 3). **NOTE that for a remote VCF file, the
`vindex` argument must be specified.** Since this VCF files is
relatively big, and thus takes longer time, we only show the code here
without evaluation.

```{r remote, eval=FALSE}
chr22url <- "ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz"
chr22url.tbi <- paste0(chr22url, ".tbi")
va <- VCFArray(chr22url, vindex =chr22url.tbi, name = "GT")
```

#### methods

We can use `vcffile()` to return the backend vcf file that is related
to each VCFArray object. And the VCFArray supports the `[`
subsetting with numeric, logical, and character subscripts. Basic
calculations are also supported by the DelayedArray.

```{r, VCFArray methods}
va <- VCFArray(file = fl, name = "GT")
vcffile(va)

va[1:3, 1:3]
va[c(TRUE, FALSE), ]
va[c("rs7410291", "rs147922003"), ]

ds <- VCFArray(fl, name = "DS")
log(ds+5)
```



## VariantExperiment: container for lazy infrastructures

### Introduction
[VariantExperiment][] is a _Bioconductor_ package that directly
extends [SummarizedExperiment][] to lazily represent very large
sequencing data (with metadata) with backends of VCF or GDS formats.

The high-throughput genetic/genomic assay data are saved as
[GDSArray][] or [VCFArray][] objects. In addition, the metadata for
features and samples (rowData and colData) are saved as
[DelayedDataFrame][] objects. So [VariantExperiment][] has enabled the
on-disk representation of both assay data and metadata. It uses
significantly less memory than in-memory R alternatives. So it is a
lightweight container for very large genomic data resources
represented as a complete experiment.

The implementation of [SummarizedExperiment][] interface enables easy
and common manipulations for high-throughput genetic/genomic data with
familiar _R/Bioconductor_ paradigms, and supports smooth
interoperability with widely used bioinformatics tools that are
available on _R/Bioconductor_.

[VariantExperiment]: https://bioconductor.org/packages/VariantExperiment
[SummarizedExperiment]: https://bioconductor.org/packages/SummarizedExperiment
[SingleCellExperiment]: https://bioconductor.org/packages/SingleCellExperiment
[SeqArray]: https://bioconductor.org/packages/SeqArray
[SeqVarTools]: https://bioconductor.org/packages/SeqVarTools

### VariantExperiment class
VariantExperiment class is defined to extend the SummarizedExperiment
class. The difference would be that the assay data are saved as
GDSArray or VCFArray, and the metadata are saved by default as
DelayedDataFrame (with option to save as ordinary DataFrame). There
are coercion methods defined for both VCF and GDS files into
VariantExperiment objects.

The coercion function of `makeVariantExperimentFromGDS()` coerces GDS
files into VariantExperiment objects directly, with the assay data
saved as GDSArray, and the rowData / colData in DelayedDataFrame by
default (with the option of ordinary DataFrame object).

```{r, VE gds constructor, eval=FALSE}
gds <- SeqArray::seqExampleFileName("gds")
ve <- makeVariantExperimentFromGDS(gds)
ve
```
```{r, VE gds accessors, eval=FALSE}
assays(ve)[[1]]
rowData(ve)
colData(ve)
```

### Constructor

Arguments in `makeVariantExperimentFromGDS()` could be specified to
take only certain annotation columns for features and samples. All
available data entries for arguments values could be retrieved by the
`showAvailable()` function with the gds file name as input.

```{r, showAvailable, eval=FALSE}
showAvailable(gds)
```

Note that the 'infoColumns' from gds file will be saved as columns
inside the rowData, with the prefix of 'info_'. 'rowDataOnDisk' and
'colDataOnDisk' could be set as 'FALSE' to save all metadata in
ordinary DataFrame format.

```{r, makeVariantExperimentFromGDSArgs, eval=FALSE}
ve3 <- makeVariantExperimentFromGDS(gds,
                                    rowDataColumns = c("ID", "ALT", "REF"),
                                    infoColumns = c("AC", "AN", "DP"),
                                    rowDataOnDisk = TRUE,
                                    colDataOnDisk = FALSE)
rowData(ve3)  #> DelayedDataFrame object
colData(ve3)  #> DataFrame object
```

If the VariantExperiment object is based on GDS backend, then the function
`gdsfile()` will return the file path to the GDS file.

```{r, VE gdsfile, eval=FALSE}
gdsfile(ve)
```

The `makeVariantExperimentFromVCF()` function (by default) converts
the VCF file into a GDS file (internally using
`SeqArray::seqVCF2GDS`), and then construct a VariantExperiment object
with the GDS file as backend.

```{r, VE vcf constructor, eval=FALSE}
vcf <- SeqArray::seqExampleFileName("vcf")
ve <- makeVariantExperimentFromVCF(vcf)
ve
gdsfile(ve)
```

Assay data is in GDSArray format, and the feature-related metadata
are in DelayedDataFrame class (with column data in GDSArray format).

```{r, VE vcf accessors, eval=FALSE}
assay(ve, 1)
rowData(ve)
```

For VCF input, Users could also have the opportunity to save the
sample related annotation directly into the VariantExperiment object,
by providing the file path to the 'sample.info' argument, and then
retrieve by `colData()`.

```{r, VE vcf sample info, eval=FALSE}
sampleInfo <- system.file("extdata", "Example_sampleInfo.txt",
                          package="VariantExperiment")
ve <- makeVariantExperimentFromVCF(vcf, sample.info = sampleInfo)
colData(ve)
```

Most of the argument are same as the
`makeVariantExperimentFromGDS`, with additional argument of 'start'
and 'count' to specify the start position and number of variants to
read into VariantExperiment object.

```{r, makeVariantExperimentFromVCFArgs_startCount, eval=FALSE}
ve2 <- makeVariantExperimentFromVCF(vcf, start=101, count=1000)
dim(ve2)
```
For the above example, only 1000 variants are read into the
VariantExperiment object, starting from the position of 101.

The support of VCFArray in the coercion method of
`makeVariantExperimentFromVCF(useVCFArray = TRUE)` is under work
now. This will read the variant call data and metadata from VCF file,
and construct into VCFArray for the assay slot and the
DelayedDataFrame for the rowData and colData slots, with columns being
VCFArray objects.

Since the VCFArray was written internally using [VariantAnnotation][],
we are also working to enable the statistical funtions and methods on
the VariantExperiment objects with VCF backends.

### Basic methods

Consistent with SummarizedExperiment, The VariantExperiment supports
`[` subsetting with numeric, logical and character indexes. The '$'
subsetting could also be operated directly on `colData()` columns, for
easy sample extraction.

- `[` and `$` subsetting

```{r, VE subset, eval=FALSE}
ve[1:10, 1:5]
ve$family
ve[, as.logical(ve$family == "1328")]  ## convert GDSArray into logical vector.
ve[as.logical(rowData(ve)$REF == "T"),]
```

- Range-based operations

VariantExperiment supports all of the `findOverlaps()` methods and
associated functions.  This includes `subsetByOverlaps()`, which makes
it easy to subset a VariantExperiment object by an interval.

```{r, VE subset by overlap, eval=FALSE}
ve1 <- subsetByOverlaps(ve, GRanges("22:1-48958933"))
ve1
```

### Save / load VariantExperiment object

Note that the operations on VariantExperiment are delayed.  So after
subsetting by '[', '$' or Ranged-based operations, and you feel
satisfied with the data for downstream analysis, you need to save that
VariantExperiment object to synchronize the on-disk file (in GDS or
VCF format) that is associated with the subset of data (in-memory
representation) before any statistical analysis. Otherwise, an error
will be returned.

For example, after we subset the 've' by 'GRanges("22:1-48958933")',
and we want to calculate the hwe based on the 23 variants, an error
will be generated indicating that we need to sync the on-disk and
in-memory representations.

```{r VEsaveLoad, eval=FALSE}
hwe(ve1)
## Error in .saveGDSMaybe(gdsfile) : use
##   'saveVariantExperiment()' to synchronize on-disk and
##   in-memory representations
```

Use the function `saveVariantExperiment()` to synchronize the on-disk
and in-memory representation, and reload into the same _R_ session.

```{r, VE save, eval=FALSE}
a <- tempfile()
ve1 <- saveVariantExperiment(ve1, dir=a, replace=TRUE)
ve1
gdsfile(ve1)
```

You can also use `loadVariantExperiment()` function to explicitly
reload any synchronized VariantExperiment object. The gds file path
to the new VariantExperiment object will be the 'se.gds' in the
specified directory.

```{r, VE load, eval=FALSE}
ve2 <- loadVariantExperiment(dir=a)
gdsfile(ve2)
```

Now we are all set for any downstream analysis as needed.

```{r, VE stats demo, eval=FALSE}
head(hwe(ve1))
```

### Statistical methods

With GDS backend, Many statistical functions and methods are defined
on VariantExperiment objects, most of which has their generic
defined in _Bioconductor_ package of [SeqArray][] and
[SeqVarTools][]. These functions could be called directly on
VariantExperiment objects as input, with additional arguments to
specify based on user's need. More details please refer to the
vignettes of [SeqArray][] and [SeqVarTools][].

Here is a list of the statistical functions with brief description:

statistical functions | Description
--------------------- | ------------
seqAlleleFreq         | Calculates the allele frequencies
seqAlleleCount        | Calculates the allele counts
seqMissing            | Calculates the missing rate for variant/sample
seqNumAllele          | Calculates the number of alleles (for ref/alt allele)
hwe                   | Exact test for Hardy-Weinberg equilibrium on Single-Nucleotide Variants
inbreedCoeff          | Calculates the inbreeding coefficient by variant/sample
pca                   | Calculates the eigenvalues and eignevectors with Principal Component Analysis
titv                  | Calculate transition/transversion ratio overall or by sample
refDosage             | Calculate the dosage of reference allele (matrix with integers of 0/1/2)
altDosage             | Calculate the dosage of alternative allele (matrix with integers of 0/1/2)
countSingletons       | Count singleton variants for each sample
heterozygosity        | Calculate heterozygosity rate by sample or by variants
homozygosity          | Calculate homozygosity rate by sample or by variants
meanBySample          | Calculate the mean value of a variable by sample over all variants
isSNV                 | Flag a single nucleotide variant
isVariant             | Locate which samples are variant for each site

Here are some examples in calculating the sample missing rate, hwe,
titv ratio and the count of singletons for each sample.

```{r, VE stats, eval=FALSE}
## sample missing rate
mr.samp <- seqMissing(ve, per.variant = FALSE)
head(mr.samp)

## hwe
hwe <- hwe(ve)
head(hwe)

## titv ratio by sample / overall
titv <- titv(ve, by.sample=TRUE)
head(titv)
titv(ve, by.sample=FALSE)

## countSingletons
countSingletons(ve)
```


## DelayedDataFrame: lazy representation of metadata

### Introduction

The development of [DelayedArray][] has made it possible for on-disk
representation of very large datasets (in different format) as
_R_-user-friendly array data structure (e.g.,
[HDF5Array][],[GDSArray][]), so that the high-throughput
genetic/genomic data are now being able to easily loaded and
manipulated within _R_. However, the metadata for the samples and
features are also getting unexpectedly larger than before. With an
ordinary data.frame or DataFrame, the memory space in _R_ faces
challenges in fast and efficient data processing, because most
available _R_ or _Bioconductor_ packages are developed based on
in-memory data manipulation.

So we have developed the DelayedDataFrame, which uses the familiar
_R/Bioconductor_ paradigm, and at the same time, all data (in the unit
of columns) could be optionally saved on-disk (e.g., in
[DelayedArray][] structure with any back-end). Common operations like
constructing, subsetting, splitting, combining could be done using the
familiarDataFrame metaphor.

This feature of DelayedDataFrame could enable efficient on-disk
reading and processing of the large-scale annotation files, and use
signicantly less memory than in-memory _R/Bioconductor_ alternatives.

[HDF5]: https://www.hdfgroup.org/solutions/hdf5/
[GDS]: http://corearray.sourceforge.net/
[DelayedArray]: https://bioconductor.org/packages/DelayedArray
[GDSArray]: https://bioconductor.org/packages/GDSArray
[HDF5Array]: https://bioconductor.org/packages/HDF5Array

### DelayedDataFrame class

DelayedDataFrame extends the DataFrame data structure, and has
similar behavior in terms of construction, subsetting, splitting,
combining, rownames setting, and etc..

The DelayedDataFrame columns supports the [DelayedArray][] (and
direct extensions). Here we use 1-dimensional GDSArray objects as
example to show the DelayedDataFrame characteristics.


```{r, GDSArray constructor}
file <- SeqArray::seqExampleFileName("gds")
gdsnodes(file)
varid <- GDSArray(file, "annotation/id")
AA <- GDSArray(file, "annotation/info/AA")
```

We use an ordinary character vector and the GDSArray objects to
construct a DelayedDataFrame object.

```{r, DDF construction}
ddf <- DelayedDataFrame(idx = seq_len(dim(AA)), varid, AA)
```

### Methods

DelayedDataFrame supports for all methods that are available in
DataFrame, including the `[` subsetting, `[[` extraction, `rbind`,
and `cbind` methods, etcs.

- subsetting

```{r ddfMethods}
ddf[, 1, drop=FALSE]  #> integer subscripts
ddf[, "AA", drop=FALSE]  #> character subscripts
ddf[, c(TRUE,FALSE)]  #> logical subscripts

ddf[["AA"]]
ddf[[3]]
```

- rbind

```{r ddfRbind}
ddf1 <- ddf[1:20,]
ddf2 <- ddf[21:40, ]
(ddfrb <- rbind(ddf1, ddf2))
```

- cbind

```{r, ddfCbind, error=FALSE}
(ddfcb <- cbind(varid = ddf[,2, drop=FALSE], AA=ddf[, 3, drop=FALSE]))
```

### DEVELOPERS ONLY

- lazyIndex slot

DelayedDataFrame uses a slot called lazyIndex (in 'LazyIndex'
class), to save all the mapping indexes for each column. The
'LazyIndex' class is defined in the DelayedDataFrame package and
extends the SimpleList class. The 'listData' slot saves unique
indexes for all the columns, and the 'index' slots saves the position
of index in 'listData' slot for each column in DelayedDataFrame
object. In the above example, with an initial construction of
DelayedDataFrame object, the index for each column will all be NULL,
and all 3 columns points the NULL values which sits in the first
position in 'listData' slot of 'lazyIndex'.

```{r}
lazyIndex(ddf)
lazyIndex(ddf)@listData
lazyIndex(ddf)@index
```

Whenever an operation is done (e.g., subsetting), the 'listData' slot
inside the DelayedDataFrame stays the same, only the lazyIndex
slot will be updated, so that the show method, further statistical
calculation will be applied to the subsetting data set.

For example, here we subset the DelayedDataFrame object `ddf` to
keep only the first 5 rows, and see how the lazyIndex works. As
shown in below, after subsetting, the 'listData' slot in `ddf1` stays
the same as `ddf`. But the subsetting operation was recorded in the
lazyIndex slot, and the slots of 'lazyIndex', 'nrows' and 'rownames'
(if not NULL) are all updated. So the subsetting operation is kind of
`delayed`.

```{r, lazyIndex}
#> ddf1 <- ddf[1:20,]
identical(ddf@listData, ddf1@listData)
lazyIndex(ddf1)
nrow(ddf1)
```

- coercion

Only when direct realization call is invoked, (e.g., `DataFrame()`, or
`as.list()`, the lazyIndex will be realized and the object of new
class returned.

For example, when DelayedDataFrame is coerced into a DataFrame object,
the 'listData' slot will be updated according to the lazyIndex slot.


```{r ddfCoercion}
df1 <- as(ddf1, "DataFrame")
df1@listData
dim(df1)
```

- rbind

NOTE that, with `rbind`-ing, the lazyIndex of input arguments will
also be realized and a new DelayedDataFrame with NULL lazyIndex will
be returned.

```{r ddfRbind_lazyIndex}
lazyIndex(ddf1)
lazyIndex(ddf2)
lazyIndex(rbind(ddf1, ddf2))
```

- cbind

While 'cbind'-ing DelayedDataFrame objects will keep all existing
lazyIndex of input arguments and carry into the new DelayedDataFrame
object.

```{r, ddfCbind_lazyIndex, error=FALSE}
lazyIndex(ddfcb)
```




## SQLDataFrame: lazy DataFrame with SQL database backend
### Introduction

A database is an organized collection of data. Relational databases is
a digital database based on the relational model of data. Virtually
all relational databases management systems use Structured Query
Language (SQL) for querying and maintaining the database.

The **rstudio** team has developed [dbplyr][], to work with remote
on-disk data stored in databases. It complies with the [dplyr][]
grammar and supports the lazy representation and manipulation of
database tables.

We are introducing an _R/Bioconductor_ package SQLDataFrame, which
was developed on top of the [dbplyr][] package. It has a DataFrame
interface, which is familiar to most _R_ users, and supports the
interoperability with the rich collection of _Bioconductor_ packages
and workflows for high-throughput genomic data analysis.

NOTE that currently we are only supporting the DBI backend of
RSQLite. But theoretically, we would implement SQLDataFrame so that
users could choose to use different database backends, including
MySQL, PostgreSQL, MariaDB, and Google's BigQuery, etc.

[dbplyr]: https://cran.r-project.org/web/packages/dbplyr/
[dplyr]: https://cran.r-project.org/web/packages/dplyr/

### SQLDataFrame class

- Constructor

To construct a SQLDataFrame object, 3 arguments are needed: 'dbname',
'dbtable' and 'dbkey'. The 'dbname' is the file path to the database
that is on-disk or remote. 'dbtable' argument specifies the database
table name that is going to be represented as SQLDataFrame object. If
only one table is available in the specified database name, this
argument could be left blank. The 'dbkey' argument is used to specify
the column name in the table which could **uniquely** identify all the
data observations (rows).

Note that after reading the database table into SQLDataFrame, the
key columns will be kept as fixed columns showing on the left hand
side, with '|' separating key column(s) with the other columns. The
'ncol', 'colnames', and corresponding column subsetting will only
apply to the non-key-columns.


```{r loadOrgdb, eval=FALSE}
org.Hs.eg()
```

```{r orgdb_construct}
dbname <- dbfile(org.Hs.eg.db)
conn <- dbconn(org.Hs.eg.db)
dbListTables(conn)

geneinfo.df <- dbReadTable(conn, "gene_info")
head(geneinfo.df)
#> check uniqueness of "dbkey"
identical(nrow(geneinfo.df), length(unique(geneinfo.df$X_id)))
#> construct SQLDataFrame
geneinfo <- SQLDataFrame(dbname = dbname, dbtable = "gene_info", dbkey = "_id")

genes.df <- dbReadTable(conn, "genes")
head(genes.df)
#> check uniqueness of "dbkey"
identical(nrow(genes.df), length(unique(genes.df$X_id)))
#> construct SQLDataFrame
genes <- SQLDataFrame(dbname, "genes", "_id")

#> join to "geneids" with gene name and symbol
geneids <- SQLDataFrame:::left_join.SQLDataFrame(genes, geneinfo)
```

- makeSQLDataframe

We could also construct a SQLDataFrame object directly from a file
name. The `makeSQLDataFrame` function takes input of a in-memory data
frame or character value of file name for common text files (.csv,
.txt, etc.), write into database tables, and open as SQLDataFrame
object. Users could provide values for the 'dbname' and 'dbtable'
argument. If NULL, default value for 'dbname' would be a temporary
database file, and 'dbtable' would be the `basename(filename)` without
extension.

**NOTE** that the input file must have one or multiple columns that
could uniquely identify each observation (row) to be used the
`dbkey()` for SQLDataFrame. Also the file must be rectangular, i.e.,
rownames are not accepted. But users could save rownames as a separate
column.

```{r}
aa <- makeSQLDataFrame(geneinfo.df, dbkey = "X_id")
aa
dbname(aa)
dbtable(aa)

filename <- file.path(tempdir(), "orgdb_geneinfo.txt")
write.table(geneinfo.df, file = filename, row.names = FALSE,
            quote = FALSE, sep="\t")
aa1 <- makeSQLDataFrame(filename, dbkey = "X_id", sep = "\t",
                       overwrite = TRUE)
aa1
dbname(aa1)
dbtable(aa1)
```

- saveSQLDataFrame

With all the methods (`[` subsetting, `rbind`, `*_join`, etc.,)
provided in the next section, the SQLDataFrame always work like a lazy
representation until users explicitly call the `saveSQLDataFrame()`
function for realization.

It's also recommended that users call `saveSQLDataFrame()` frequently
to avoid the slow down in data processing because of too many lazy
layers.

```{r saveSQLDF}
dbfile <- system.file("extdata/test.db", package = "SQLDataFrame")
obj <- SQLDataFrame(dbname = dbfile, dbtable = "state", dbkey = "state")
dbname(obj)
dbtable(obj)
obj1 <- saveSQLDataFrame(obj, dbname = tempfile(fileext = ".db"),
                         dbtable = "obj_copy")
dbname(obj1)
dbtable(obj1)
```


### SQLDataFrame methods

#### [, [[

`[[,SQLDataFrame` Behaves similarly to `[[,DataFrame` and returns a
realized vector of values from a single column. `$,SQLDataFrame` is
also defined to conveniently extract column values.

```{r}
head(geneids[[2]])
head(geneids[["symbol"]])
```

SQLDataFrame objects can be subsetted in a similar way of
DataFrame following the usual _R_ conventions, with numeric,
character or logical vectors; logical vectors are recycled to the
appropriate length.

**NOTE**, use `drop=FALSE` explicitly for single column subsetting if
you want to return a SQLDataFrame object, otherwise, the default
`drop=TRUE` would always return a realized value for that column.

```{r, subsetting}
geneids[sample(100, 5), 1:2]
geneids[c(TRUE, FALSE), "gene_name", drop=FALSE]
geneids[c(1, 5, 9), ]
```

List style subsetting is also allowed to extract certain columns from
the SQLDataFrame object which returns SQLDataFrame by default.

```{r}
geneids[2]  #> list style subsetting
geneids["symbol"]
```

#### filter, mutate

We have also enabled the S3 methods of `filter` and `mutate` from
[dplyr][] package, so that users could have the convenience in filtering
data observations and adding new columns.

```{r}
geneids %>% filter(symbol %in% c("AANAT", "ABCA3"))
geneids %>% mutate(symbol1 = paste0(symbol, "_dup"))
```

#### union, rbind

To be consistent with DataFrame, `union` and `rbind` methods were
implemented for SQLDataFrame, where `union` returns the
SQLDataFrame sorted by the `dbkey(obj)` and unique rows, and `rbind`
keeps the original orders of input objects.

```{r SQLDF_unionRbind}
union(geneids[sample(5),], geneids[sample(10,5), ])  #> unique & sorted
rbind(geneids[sample(5),], geneids[sample(10,5), ])
```

#### *_join

The `*_join` family methods was implemented for SQLDataFrame
objects, including the `left_join`, `inner_join`, `semi_join` and
`anti_join`, which provides the capability of merging database files
from different sources.

Here we use a simple example to show that how to use the gene ids to
match the transcription ids using `*_join` functions, and return a
table with matching positions for each transcripts.

```{r SQLDF_tx}
dbname.tx <- dbfile(TxDb.Hsapiens.UCSC.hg38.knownGene)
conn.tx <- dbconn(TxDb.Hsapiens.UCSC.hg38.knownGene)
dbListTables(conn.tx)

txmap <- SQLDataFrame(dbname = dbname.tx,
                      dbtable = "gene",
                      dbkey = "_tx_id")
txchrom <- SQLDataFrame(dbname = dbname.tx,
                        dbtable = "transcript",
                        dbkey = "_tx_id")
txinfo <- left_join(txmap, txchrom)
txinfo
geneids
res <- inner_join(geneids, txinfo)
```

### DEVELOPERS ONLY

To make the SQLDataFrame object as light and compact as possible,
there are only 5 slots contained in the object: tblData, dbkey,
dbnrows, dbconcatKey, indexes. Metadata information could be returned
through these 5 slots using slot accessors or other utility functions.

```{r, SQLDF_developers}
slotNames(res)
dbname(res)
dbtable(res)
dbkey(res)
```

- tblData slot

The tblData slot saves the `dbplyr::tbl_dbi` version of the database
table, which is a light-weight representation of the database table in
_R_. Of note is that this lazy tbl only contains unique rows. It could
also be sorted by the dbkey if the SQLDataFrame object was generated
from `union()` or `rbind()`. So when the `saveSQLDataFrame()` function
was called, a database table will be written into a physical disk
space and have the unique records.

Accessor function is made avaible for this slot:
```{r}
tblData(res)
```

`saveSQLDataFrame()` write the lazy tbl carried in tblData slot into
an on-disk database table, and re-open the SQLDataFrame object from
the new path.

```{r saveSQLDataFrame}
dbname(res)
dbtable(res)
res <- saveSQLDataFrame(res, dbname = tempfile(fileext=".db"),
                        dbtable = "gene_tx")
dbname(res)
dbtable(res)
```

- dbnrows and dbconcatKey

The dbnrows slot saves the number of rows corresponding to the
tblData, and dbconcatKey saves the realized (concatenated if multiple)
key columns corresponding to the tblData. Accessor functions are also
available for these 2 slots:

```{r}
dbnrows(res)
head(dbconcatKey(res))
```

- indexes slot

The indexes slots is an unnamed list saving the row and column indexes
respectively corresponding to the tblData, so that the SQLDataFrame
could possibly have duplicate rows or only a subset of data records
from the tblData, while the tblData slot doesn't need to be
changed. To be consistent, the slots of dbnrows and dbconcatKey will
also remain unchanged.

```{r}
res@indexes
res_sub <- res[sample(100, 5, replace = TRUE),
               c("symbol", "tx_name", "tx_start")]
res_sub
res_sub@indexes
identical(tblData(res), tblData(res_sub))
```

With a `filter()` function (which is similar to [i, ] subsetting),
only the indexes slot will be updated for the row index pointing to
the tblData.

```{r}
res_filter <- res %>% filter(symbol == "A1BG")
res_filter@indexes
identical(tblData(res), tblData(res_filter))
```

- methods

The `ROWNAMES,SQLDataFrame` method was defined to return the
(concatenated by '\b' if multiple) key column(s) value, so that the
row subsetting with character vector works for the SQLDataFrame
resects.

```{r}
head(ROWNAMES(res))
res[sample(ROWNAMES(res), 5), ]
```



## Future directions


### DelayedArray and related

- Store the dimnames in the HDF5 file.

- DelayedArray is still a work-in-progress (we need more efficient
  block-processing and RleArray objects).

- In particular, DelayedArray backend authors should be able to
  implement optimized backend-specific methods (e.g. `max`) and
  the methods for DelayedArray objects should try to take advantage
  of them instead of triggering block processing.

- Support other on-disk DelayedArray backends (TileDB?)


### On-disk DataFrame

- Explore other on-disk DataFrame-like objects (Spark?)

- DataFrame-like classes like SQLDataFrame don't extend DataFrame at
  the moment:

    ```{r}
    showClass("SQLDataFrame")
    ```

    This prevents these objects from being used to store the metadata
    columns of Vector derivatives.


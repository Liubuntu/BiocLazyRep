---
author:
- name: Hervé Pagès
  affiliation: Fred Hutchinson Cancer Research Center, Seattle, WA
- name: Qian Liu
  affiliation: Roswell Park Comprehensive Cancer Center, Buffalo, NY
- name: Martin Morgan
  affiliation: Roswell Park Comprehensive Cancer Center, Buffalo, NY
vignette: >
  %\VignetteIndexEntry{Lazy Representations of Very Large Genomic Data Resources in R/Bioconductor}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}{inputenc}
date: "Last modified: May 30, 2019; Compiled: `r format(Sys.time(), '%B %d, %Y')`"
output:
    rmarkdown::html_document:
    highlight: pygments
    toc: true
    toc_depth: 3
    fig_width: 5
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
```{r options, include = FALSE}
options(showHeadLines=3)
options(showTailLines=3)
```

# Lazy Representations of Very Large Genomic Data Resources in R/Bioconductor

## Instructors and contact information

- **Hervé Pagès**, Fred Hutchinson Cancer Research Center, Seattle, WA
- **Qian Liu**, Roswell Park Comprehensive Cancer Center, Buffalo, NY
- **Martin Morgan**, Roswell Park Comprehensive Cancer Center, Buffalo, NY

## Workshop Description

In this workshop, we will learn the lazy representations and
interfaces in _R/Bioconductor_, as well as the application in the real
data analysis. We will use some real data examples generated from
DNA/RNA-seq, to demonstrate the representation and comprehension of
large scale (out-of-memory) genomic datasets. The workshop will be
mainly instructor-led live demo with completely working
examples. Instructions and notes will be included.

## Pre-requisites

- Basic knowledge of R syntax (`matrix`, `array`, `data.frame`, etc.)
- Knowledge of the `DelayedArray` class (Hervé?)
- Familiarity with `SummarizedExperiment` class

## Workshop Participation

Students will be using their laptops with internet connection, follow
the instructor to read course materials and run through the working
code chunks.

## Time outline

Activity | Time 
---------|------
DelayedArray and about   | 50m  (**to be modified by Hervé**)
Specialized DelayedArray backends | 14m
DelayedDataFrame | 10m 
SQLDataFrame (&examples?) | 10m
VariantExperiment & examples | 16m

## Learning objectives
- Create `DelayedArray` with different backends (**to be modified by Hervé**)
- Use 
- Create `SQLDataFrame` from in-memory data, text format file, and database tables. 
- Use `SQLDataFrame` to represent and manipulate godb database tables.
- Statistical analysis of variant data (VCF) using `VariantExperiment`.
		
##  _R/Bioconductor_ packages 

to-be-used in this workshop:

```{r setup, message = FALSE}
library(DelayedArray)
library(HDF5Array)
library(DelayedMatrixStats)
library(SummarizedExperiment)
library(gdsfmt)
library(SeqArray)
library(GDSArray)
library(VCFArray)
library(DelayedDataFrame)
library(SQLDataFrame) ## depends on "dplyr", and "dbplyr"
library(VariantAnnotation)
## library(VariantExperiment)  ## depends on "DelayedDataFrame", and "GDSArray"
```



## DelayedArray objects


### What is a DelayedArray object?

A DelayedArray object is a data structure that can be used to wrap any
matrix-like or array-like dataset. This wrapping serves 2 purposes:
(1) it allows to reduce considerably the amount of memory needed to
perform common matrix or array operations on the dataset, and (2) it
allows operating on the dataset while treating it as read-only.

The DelayedArray class is an S4 class defined in the _DelayedArray_
package. A simple way to create a DelayedArray object is to call the
`DelayedArray()` constructor on a matrix-like or array-like object:

```{r, DelayedArray}
library(DelayedArray)

m <- matrix(1:30, nrow=5)
M <- DelayedArray(m)
M

a <- array(runif(15000000), dim=c(10000, 300, 5))
A <- DelayedArray(a)
A
```

See `?DelayedArray` for more information about the DelayedArray constructor.

The matrix-like or array-like object passed to the constructor is called
the _seed_ of the DelayedArray object.

The seed can be an in-memory dataset (e.g. an ordinary matrix or array, or
a dgCMatrix object from the _Matrix_ package) or an on-disk dataset (e.g.
an HDF5 dataset).

Here is an example of a DelayedArray object where the seed is an HDF5 dataset:

```{r}
library(HDF5Array)
A2 <- as(a, "HDF5Array")   # write 'a' to an HDF5 file
A2
```

An HDF5Array is a particular kind of DelayedArray object:

```{r}
is(A2, "DelayedArray")
```

`A2`'s memory footprint is very small because the data is on disk:

```{r}
object.size(A2)
object.size(a)
```

Think of `A2` as a pointer to the HDF5 dataset. To see where `A2` is pointing
at, look at its seed:

```{r}
seed(A2)                   # an HDF5ArraySeed object
```

DelayedArray objects support many common matrix or array operations:

```{r}
A3 <- log(A2 + 1)
A3

sum(A3)
```


### Operations on DelayedArray objects

Operations on a DelayedArray object are either `delayed` or `block-processed`.

#### Delayed operations

For example the `+ 1` and `log` operations in `A3 <- log(A2 + 1)` are delayed.
This means that they are very fast and produce a new DelayedArray object
with the same seed as the input object:

```{r}
seed(A3)

identical(seed(A2), seed(A3))
```

Note that `A3`'s memory footprint is still very small because all the data
is still on disk:
```
object.size(A3)

showtree(A3)
```

The only difference between `A2` and `A3` is that `A3` now carries delayed
operations.

We could keep going:

```{r}
M <- t(A3[ , 1:20, 5])
M
```

The seed is still the same. This only adds more delayed operations to be
applied to it:

```{r}
seed(M)

object.size(M)

showtree(M)
```

An important principle: The seed of a DelayedArray object is **always**
treated as a _read-only_ object so will never be modified by the operations
we perform on the object.

#### Block-processed operations

TODO (by Hervé)

- Refer to _DelayedMatrixStats_ package for more matrix summarization
  operations on DelayedMatrix objects.


### In-memory vs on-disk objects

TODO (by Hervé)


### HDF5 backend

TODO (by Hervé)
- `path(A2)`
- `h5ls(path(A2))`


### Using DelayedArray objects in a SummarizedExperiment object

TODO (by Hervé)


### Implementing a DelayedArray backend

TODO (by Hervé)


### Conclusion

TODO (by Hervé)

- Don't miss the "Effectively using the DelayedArray framework to support
  the analysis of large datasets" workshop by Pete Hickey if you are
  planning to use DelayedArray objects in your package.



## Specialized DelayedArray backends



### Extension of `DelayedArray` with GDS backend

#### CoreArray and Genomic Data Structure

[CoreArray][] (C++ library) is designed for portable and scalable
storage of large-scale bioinformatics data, which are usually much
larger than the available random-access memory. The data format of
Genomic Data Structure (GDS) is used to store multiple array-oriented
datasets in a single file. The _Bioconductor_ package [gdsfmt][] has
provided a high-level R interface for [CoreArray][].

The GDS format has been widely used in genetic/genomic research for
high-throughput genotyping or sequencing data.  There are two major S3
classes that extends the basic `gds.class`: `SNPGDSFileClass` (defined
in [SNPRelate][]) suited for genotyping data (e.g., GWAS), and
`SeqVarGDSClass` (defined in [SeqArray][]) that are designed
specifically for DNA-sequencing data. The file format attribute in
each data class is set as `SNP_ARRAY` and `SEQ_ARRAY`. There are rich
functions written based on these data classes for common data
operation and statistical analysis in the R packages of [SNPRelate][]
and [SeqArray][].

More details about GDS can be found in the above mentioned package
vignettes. Additional information about CoreArray and GDS could also
be found [here](https://www.biostat.washington.edu/sites/default/files/modules/GDS_intro.pdf).

[CoreArray]: http://corearray.sourceforge.net/
[gdsfmt]: https://bioconductor.org/packages/gdsfmt
[SNPRelate]: https://bioconductor.org/packages/SNPRelate
[SeqArray]: https://bioconductor.org/packages/SeqArray

The GDS file inside R uses hierarchical structure: 

```{r, GDSopen}
file <- seqExampleFileName("gds")
g1 <- gdsfmt::openfn.gds(file)
g1
```

Specific functions in manipulating the GDS file: 

Package | Function | Description
--------|----------|--------------
gdsfmt | createfn.gds | Create a gds file
gdsfmt | openfn.gds | open a gds file in R
gdsfmt | add.gdsn | add a gds node
gdsfmt | assign.gdsn | assign values into gds node
gdsfmt | closefn.gds | close a gds instance
SeqArray | seqVCF2GDS | Reformat a VCF file
SeqArray | seqSetFilter | Define a subset of samples or variants
SeqArray | seqGetData | Get data with a defined filter
SeqArray | seqApply | Apply a user-defined function over array margins
SeqArray | seqParallel | Apply a function in parallel

For example, `index.gdsn` will be used to refer to a gds node, and
`readex.gdsn` is the function to call for subset reading, by providing
the index node, and the indexes as a list.

```{r, GDSread}
sample.node <- index.gdsn(g1, "sample.id")
readex.gdsn(sample.node, 1:5)
geno.node <- index.gdsn(g1, "genotype/data")
aperm(
    readex.gdsn(geno.node, list(1, 1:5, 1:5)),
    c(3,2,1))
closefn.gds(g1)
```

These functions corresponds to the GDS data structure and works best
 with the indexing strategy and compression methods. But at the same
 time, These usual operations on manipulating files and contents
 require much efforts from users in order to make full use of these
 packages.

#### `GDSArray`, `GDSMatrix`, and `GDSFile`

To fully make use of the GDS format, and at the same time, to provide
users a familiar interface with common operations and methods within
the _R/Bioconductor_ ecosystem, We have developed the package of
[GDSArray][] to represent GDS files as `DelayedArray` instances. It
obeys the seed contract of [DelayedArray][] by supporting `dim`,
`dimnames` methods, and inherits array-like operations and methods
from `DelayedArray`, e.g., the subsetting method of `[`.

The `GDSArray()` constructor takes arguments for the GDS file path and
the GDS node name, with one `GDSArray` object only representing one
GDS node. The `GDSArray()` built from GDS file always returns with
rows being "features" (genes / variants / snps) and the columns being
"samples", which is consistent with the assay data inside
`SummarizedExperiment` and easy to fit into a `SummarizedExperiment`
object.

```{r, GDSArray}
file <- SeqArray::seqExampleFileName("gds")
GDSArray(file, "genotype/data")
```
Note that a `GDSMatrix` will be returned by default for 2-dimensional
`GDSArray`. 

```{r, GDSMatrix}
GDSArray(file, "phase/data")
```

The `GDSFile` is a light-weight class to represent GDS files. It has
the `$` completion method to complete any possible gds nodes. It could
be used as a convenient `GDSArray` constructor if the slot of
`current_path` in `GDSFile` object represents a valid gds node.
Otherwise, it will return the `GDSFile` object with an updated
`current_path`.

```{r, GDSFile}
gf <- GDSFile(file)
gf$annotation$info
gf$annotation$info$AC
```
Try typing in `gf$ann` and pressing `tab` key for the
auto-completion. (Changes made in `GDSArray`, wait for new build
tomorrow June 7th.)

`gdsfile` function returns the file path of the corresponding GDS
  file.
```{r, gdsfileAccessor}
gdsfile(gf)
```

#### seed contract

`GDSArray` follows the seed contract for `DelayedArray` extension, and
  supports the `dim`, `dimnames` methods.

```{r, GDSArray seed contract}
ga <- GDSArray(file, "genotype/data")
seed(ga)
dim(ga)
lengths(dimnames(ga))
```

#### methods

`GDSArray` instances can be subset, following the usual _R_
conventions, with numeric or logical vectors; logical vectors are
recycled to the appropriate length.

```{r, GDSArray methods}
ga[1:3, 10:15, ]
ga[c(TRUE, FALSE), , ]
```

some numeric calculation: 
```{r, GDSArray numeric}
dp <- GDSArray(file, "annotation/format/DP/data")
log(dp)
dp[rowMeans(dp) < 60, ]
```

**Summary:**

`GDSArray` represents GDS file as objects derived from `DelayedArray`
class. It converts a GDS node in the file to a `DelayedArray` derived
data structure. The rich common methods and data operations defined on
`GDSArray` makes it more R-user-friendly than working with the GDS
file directly.

The array data from GDS files are always returned with the first
dimension being "variants/snps" and the second dimension being
"samples". This feature is consistent with the assay data saved in
`SummarizedExperiment`, and makes the `GDSArray` package easily
interoperable with other established _Bioconductor_ data
infrastructure and methods.



### Extension of `DelayedArray` with VCF backend
	

#### Introduction

The Variant Call Format (VCF) was designed to store gene sequence
variations for the large-scale genotyping and DNA sequencing projects,
(e.g., the 1000 Genomes Project). With the increasing sample size and
sequencing depth, the VCF file also gets much larger than ever and
hardly fit the memory limit of _R_.

[VCFArray][] is a _Bioconductor_ package that represents VCF data
entries as objects derived from the [DelayedArray][] data
structure. The backend VCF file could either be saved on-disk locally
or remote as online resources. Supported data entries include the
fixed data fields (REF, ALT, QUAL, FILTER), information field (e.g.,
AA, AF...), and the individual format field (e.g., GT, DP, etc.). Full
list of supported data entries could be retrieved by `vcfFields`
method (see details below).

The array data generated from fixed/information fields are
one-dimensional`VCFArray`, with the dimension being the length of the
variants. The array data generated from individual `FORMAT` field are
always returned with the first dimension being `variants` and the
second dimension being `samples`. This feature is consistent with the
assay data saved in `SummarizedExperiment`, and makes the `VCFArray`
package easily interoperable with other established _Bioconductor_
data infrastructure.
 
[VCFArray]: https://bioconductor.org/packages/VCFArray

#### `vcfFields()`

The `vcfFields()` method takes the character string (VCF file path),
`VcfFile` object or `RangedVcfStack` object as input, and returns a
CharacterList with all available VCF fields within specific
categories. Users should consult the `fixed`, `info` and `geno`
category for available data entries that could be converted into
`VCFArray` instances. The data entry names can be used as input for
the `name` argument in `VCFArray` constructor.

```{r avail, message=FALSE}
args(VCFArray)
fl <- system.file("extdata", "chr22.vcf.gz", package = "VariantAnnotation")
library(VariantAnnotation)
vcfFields(fl)
```

#### `VCFArray`, `VCFMatrix` and `vcffile()`

We can construct the `VCFArray` object from the same input as
`vcfFields()` methods (the character string for VCF file path,
`VcfFile` object or `RangedVcfStack` object). 

With a simplest example, we can construct a `VCFArray` object for the
`GT` data entry in the provided VCF file with arguments of `file` and
`name` only.

```{r VCFArray constructor}
## character string
VCFArray(file = fl, name = "GT")

## "VcfFile"
vcf <- VariantAnnotation::VcfFile(fl)
VCFArray(file = vcf, name = "DS")

## "RangedVcfStack"
extdata <- system.file(package = "GenomicFiles", "extdata")
files <- dir(extdata, pattern="^CEUtrio.*bgz$", full=TRUE)[1:2]
names(files) <- sub(".*_([0-9XY]+).*", "\\1", basename(files))
seqinfo <- as(readRDS(file.path(extdata, "seqinfo.rds")), "Seqinfo")
stack <- GenomicFiles::VcfStack(files, seqinfo)
gr <- as(GenomicFiles::seqinfo(stack)[rownames(stack)], "GRanges")
## RangedVcfStack
rgstack <- GenomicFiles::RangedVcfStack(stack, rowRanges = gr)
rgstack

vcfFields(rgstack)$geno
VCFArray(rgstack, name = "SB")
```

the backend VCF file could also be remote files. Here we included an
example of representing VCF file of chromosome 22 from the 1000
Genomes Project (Phase 3). **NOTE that for a remote VCF file, the
`vindex` argument must be specified.** Since this VCF files is
relatively big, and thus takes longer time, we only show the code here
without evaluation.

```{r remote, eval=FALSE}
chr22url <- "ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz"
chr22url.tbi <- paste0(chr22url, ".tbi")
va <- VCFArray(chr22url, vindex =chr22url.tbi, name = "GT")
```

#### `DelayedArray` seed contract

The `VCFArray` complies the seed contract of the `DelayedArray` by
supporting the `dim`, `dimnames` methods. 

```{r, VCFArray seedAccessor}
va <- VCFArray(fl, name = "GT")
seed(va)

dim(va)
lapply(dimnames(va), head)
```

#### methods

We can use `vcffile()` to return the backend vcf file that is related
to each `VCFArray` object. And the `VCFArray` supports the `[`
subsetting with numeric, logical, and character subscripts. Basic
calculations are also supported by the `DelayedArray`.

```{r, VCFArray methods}
vcffile(va)

va[1:3, 1:3]
va[c(TRUE, FALSE), ]
va[c("rs7410291", "rs147922003"), ]

ds <- VCFArray(fl, name = "DS")
log(ds+5)
```



## `DelayedDataFrame` for delayed representation of metadata 
	
### Introduction

The development of [DelayedArray][] has made it possible for on-disk
representation of very large datasets (in different format) as
_R_-user-friendly array data structure (e.g.,
[HDF5Array][],[GDSArray][]), so that the high-throughput
genetic/genomic data are now being able to easily loaded and
manipulated within _R_. However, the metadata for the samples and
features are also getting unexpectedly larger than before. With an
ordinary `data.frame` or `DataFrame`, the memory space in _R_ faces
challenges in fast and efficient data processing, because most
available _R_ or _Bioconductor_ packages are developed based on
in-memory data manipulation.

So we have developed the `DelayedDataFrame`, which uses the familiar
_R/Bioconductor_ paradigm, and at the same time, all data (in the unit
of columns) could be optionally saved on-disk (e.g., in
[DelayedArray][] structure with any back-end). Common operations like
constructing, subsetting, splitting, combining could be done using the
familiar`DataFrame` metaphor. 

This feature of `DelayedDataFrame` could enable efficient on-disk
reading and processing of the large-scale annotation files, and use
signicantly less memory than in-memory _R/Bioconductor_ alternatives.

[HDF5]: https://www.hdfgroup.org/solutions/hdf5/
[GDS]: http://corearray.sourceforge.net/
[DelayedArray]: https://bioconductor.org/packages/DelayedArray
[GDSArray]: https://bioconductor.org/packages/GDSArray
[HDF5Array]: https://bioconductor.org/packages/HDF5Array
	
### `DelayedDataFrame` class

`DelayedDataFrame` extends the `DataFrame` data structure, and has
similar behavior in terms of construction, subsetting, splitting,
combining, rownames setting, and etc.. 

The `DelayedDataFrame` columns supports the [DelayedArray][] (and
direct extensions). Here we use 1-dimensional `GDSArray` objects as
example to show the `DelayedDataFrame` characteristics.


```{r, GDSArray constructor}
file <- SeqArray::seqExampleFileName("gds")
gdsnodes(file)
varid <- GDSArray(file, "annotation/id")
AA <- GDSArray(file, "annotation/info/AA")
```

We use an ordinary character vector and the `GDSArray` objects to
construct a `DelayedDataFrame` object.

```{r, DDF construction}
ddf <- DelayedDataFrame(idx = seq_len(dim(AA)), varid, AA)
```

### Methods

`DelayedDataFrame` supports for all methods that are available in
`DataFrame`, including the `[` subsetting, `[[` extraction, `rbind`,
and `cbind` methods, etcs.

- subsetting

```{r ddfMethods}
ddf[, 1, drop=FALSE]  ## integer subscripts
ddf[, "AA", drop=FALSE]  ## character subscripts
ddf[, c(TRUE,FALSE)]  ## logical subscripts

ddf[["AA"]]
ddf[[3]]
```

- rbind

```{r ddfRbind}
ddf2 <- ddf[21:40, ]
(ddfrb <- rbind(ddf, ddf2))
```

- cbind

```{r, ddfCbind, error=FALSE}
(ddfcb <- cbind(varid = ddf[,2, drop=FALSE], AA=ddf[, 3, drop=FALSE]))
```

### DEVELOPERS ONLY: `lazyIndex` slot

`DelayedDataFrame` uses a slot called `lazyIndex` (in `LazyIndex`
class), to save all the mapping indexes for each column. The
`LazyIndex` class is defined in the `DelayedDataFrame` package and
extends the `SimpleList` class. The `listData` slot saves unique
indexes for all the columns, and the `index` slots saves the position
of index in `listData` slot for each column in `DelayedDataFrame`
object. In the above example, with an initial construction of
`DelayedDataFrame` object, the index for each column will all be NULL,
and all 3 columns points the NULL values which sits in the first
position in `listData` slot of `lazyIndex`.

```{r}
lazyIndex(ddf)
lazyIndex(ddf)@listData
lazyIndex(ddf)@index
```

Whenever an operation is done (e.g., subsetting), the `listData` slot
inside the `DelayedDataFrame` stays the same, only the `lazyIndex`
slot will be updated, so that the show method, further statistical
calculation will be applied to the subsetting data set. 

For example, here we subset the `DelayedDataFrame` object `ddf` to
keep only the first 5 rows, and see how the `lazyIndex` works. As
shown in below, after subsetting, the `listData` slot in `ddf1` stays
the same as `ddf`. But the subsetting operation was recorded in the
`lazyIndex` slot, and the slots of `lazyIndex`, `nrows` and `rownames`
(if not NULL) are all updated. So the subsetting operation is kind of
`delayed`.

```{r, lazyIndex}
ddf1 <- ddf[1:20,]
identical(ddf@listData, ddf1@listData)
lazyIndex(ddf1)
nrow(ddf1)
```

Only when direct realization call is invoked, (e.g., `DataFrame()`, or
`as.list()`, the `lazyIndex` will be realized and the object of new
class returned. 

For example, when `DelayedDataFrame` is coerced into a `DataFrame`
object, the `listData` slot will be updated according to the
`lazyIndex` slot.


```{r ddfCoercion}
df1 <- as(ddf1, "DataFrame")
df1@listData
dim(df1)
```

NOTE that, with `rbind`-ing, the `lazyIndex` of input arguments will
also be realized and a new `DelayedDataFrame` with NULL lazyIndex will
be returned.

```{r ddfRbind2}
lazyIndex(ddf1)
lazyIndex(ddf2)
lazyIndex(rbind(ddf1, ddf2))
```

While `cbind`-ing of`DelayedDataFrame` objects will keep all existing
`lazyIndex` of input arguments and carry into the new
`DelayedDataFrame` object.

```{r, ddfCbind2, error=FALSE}
lazyIndex(ddfcb)
```




## SQLDataFrame
### Introduction

A database is an organized collection of data. Relational databases is
a digital database based on the relational model of data. Virtually
all relational databases management systems use Structured Query
Language (SQL) for querying and maintaining the database.

The **rstudio** team has developed `dbplyr`, to work with remote
on-disk data stored in databases. It complies with the `dplyr` grammar
and supports the lazy representation and manipulation of database
tables.

We are introducing an _R/Bioconductor_ package `SQLDataFrame`, which
was developed on top of the `dbplyr` package. It has a `DataFrame`
interface, which is familiar to most _R_ users, and supports the
interoperability with the rich collection of _Bioconductor_ packages
and workflows for high-throughput genomic data analysis.

NOTE that currently we are only supporting the DBI backend of
`RSQLite`. But theoretically, we would implement `SQLDataFrame` so
that users could choose to use different database backends, including
MySQL, PostgreSQL, MariaDB, and Google's BigQuery, etc. 

### SQLDataFrame class

- Constructor

To construct a `SQLDataFrame` object, 3 arguments are needed:
`dbname`, `dbtable` and `dbkey`. The `dbname` is the file path to the
database that is on-disk or remote. `dbtable` argument specifies the
database table name that is going to be represented as `SQLDataFrame`
object. If only one table is available in the specified database name,
this argument could be left blank. The `dbkey` argument is used to
specify the column name in the table which could uniquely identify all
the data observations (rows). 

Note that after reading the database table into `SQLDataFrame`, the
key columns will be kept as fixed columns showing on the left hand
side, with `|` separating key column(s) with the other columns. The
`ncol`, `colnames`, and corresponding column subsetting will only
apply to the non-key-columns.


```{r constructor}
dbfile <- system.file("extdata/test.db", package = "SQLDataFrame")
obj <- SQLDataFrame(
    dbname = dbfile, dbtable = "colData", dbkey = "sampleID")
obj
dim(obj)
colnames(obj)
```

- makeSQLDataframe

We could also construct a `SQLDataFrame` object directly from a file
name. The `makeSQLDataFrame` function takes input of a in-memory data
frame or character value of file name for common text files (.csv,
.txt, etc.), write into database tables, and open as `SQLDataFrame`
object. Users could provide values for the `dbname` and `dbtable`
argument. If NULL, default value for `dbname` would be a temporary
database file, and `dbtable` would be the `basename(filename)` without
extension.

**NOTE** that the input file must have one or multiple columns that
could uniquely identify each observation (row) to be used the
`dbkey()` for `SQLDataFrame`. Also the file must be rectangular, i.e.,
rownames are not accepted. But users could save rownames as a separate
column. 

```{r}
mtc <- tibble::rownames_to_column(mtcars)[,1:6]
aa <- makeSQLDataFrame(mtc, dbkey = "rowname", overwrite = TRUE)
aa
dbname(aa)
dbtable(aa)

filename <- file.path(tempdir(), "mtc.csv")
write.csv(mtc, file= filename, row.names = FALSE)
aa1 <- makeSQLDataFrame(filename, dbkey = "rowname", sep = ",",
                       overwrite = TRUE)
aa1
dbname(aa1)
dbtable(aa1)
```

- saveSQLDataFrame

With all the methods (`[` subsetting, `rbind`, `*_join`, etc.,)
provided in the next section, the `SQLDataFrame` always work like a
lazy representation until users explicitly call the `saveSQLDataFrame`
function for realization. 

It's also recommended that users call `saveSQLDataFrame` frequently to
avoid the slow down in data processing because of too many lazy layers. 

```{r}
dbfile <- system.file("extdata/test.db", package = "SQLDataFrame")
obj <- SQLDataFrame(dbname = dbfile, dbtable = "state", dbkey = "state")
dbname(obj)
dbtable(obj)
obj1 <- saveSQLDataFrame(obj, dbname = tempfile(fileext = ".db"),
                         dbtable = "obj_copy")
dbname(obj1)
dbtable(obj1)
```


### SQLDataFrame methods

#### [, [[

`[[,SQLDataFrame` Behaves similarly to `[[,DataFrame` and returns a
realized vector of values from a single column. `$,SQLDataFrame` is
also defined to conveniently extract column values.

```{r}
head(obj[[1]])
head(obj[["region"]])
head(obj$size)
```
`SQLDataFrame` objects can be subsetted in a similar way of
`DataFrame` following the usual _R_ conventions, with numeric,
character or logical vectors; logical vectors are recycled to the
appropriate length. 

**NOTE**, use `drop=FALSE` explicitly for single column subsetting if
you want to return a `SQLDataFrame` object, otherwise, the default
`drop=TRUE` would always return a realized value for that column.

```{r, subsetting}
obj[1:3, 1:2]
obj[c(TRUE, FALSE), c(TRUE, FALSE), drop=FALSE]
obj[1:3, "population", drop=FALSE]
obj[, "population"]  ## realized column value
obj[c("Alabama", "Colorado"), ]
```

List style subsetting is also allowed to extract certain columns from
the `SQLDataFrame` object which returns `SQLDataFrame` by default. 

```{r}
obj[1]
obj["region"]
```

#### filter, mutate

We have also enabled the S3 methods of `filter` and `mutate` from
`dplyr` package, so that users could have the convenience in filtering
data observations and adding new columns.

```{r}
obj1 %>% filter(division == "South Atlantic" & size == "medium")
```

```{r}
obj1 %>% mutate(p1 = population/10, s1 = size)
```

#### union, rbind

To be consistent with `DataFrame`, `union` and `rbind` methods were
implemented for `SQLDataFrame`, where `union` returns the
`SQLDataFrame` sorted by the `dbkey(obj)`, and `rbind` keeps the
original orders of input objects.

```{r}
dbfile1 <- system.file("extdata/test.db", package = "SQLDataFrame")
dbfile2 <- system.file("extdata/test1.db", package = "SQLDataFrame")
ss1 <- SQLDataFrame(dbname = dbfile1, dbtable = "state",
                    dbkey = c("state"))
ss2 <- SQLDataFrame(dbname = dbfile2, dbtable = "state1",
                    dbkey = c("state"))
ss11 <- ss1[sample(5), ]
ss21 <- ss2[sample(10, 5), ]
```

- union
```{r, eval=FALSE}
obj1 <- union(ss11, ss21)
obj1  ## reordered by the "dbkey()"
```

- rbind
```{r}
obj2 <- rbind(ss11, ss21)
obj2  ## keeping the original order by updating the row index
```

#### *_join

The `*_join` family methods was implemented for `SQLDataFrame`
objects, including the `left_join`, `inner_join`, `semi_join` and
`anti_join`, which provides the capability of merging database files
from different sources.

```{r}
ss12 <- ss1[1:10, 1:2]
ss22 <- ss2[6:15, 3:4]
left_join(ss12, ss22)
inner_join(ss12, ss22)
semi_join(ss12, ss22)
anti_join(ss12, ss22)
```

### DEVELOPERS ONLY

To make the `SQLDataFrame` object as light and compact as possible,
there are only 5 slots contained in the object: `tblData`, `dbkey`,
`dbnrows`, `dbconcatKey`, `indexes`. Metadata information could be
returned through these 5 slots using slot accessors or other utility
functions. 
```{r}
slotNames(obj)
dbname(obj)
dbtable(obj)
dbkey(obj)
```

- `tblData` slot

The `tblData` slot saves the `dbplyr::tbl_dbi` version of the database
table, which is a light-weight representation of the database table in
_R_. Of note is that this lazy tbl only contains unique rows. It could
also be sorted by the `dbkey(obj)` if the `SQLDataFrame` object was
generated from `union` or `rbind`. So when the `saveSQLDataFrame()`
function was called, a database table will be written into a physical
disk space and have the unique records.

Accessor function is made avaible for this slot: 
```{r}
tblData(obj)
```

`saveSQLDataFrame` write the lazy tbl carried in `tblData` slot into
an on-disk database table, and re-open the `SQLDataFrame` object from
the new path.


- `dbnrows` and `dbconcatKey`
The `dbnrows` slot saves the number of rows corresponding to the
`tblData`, and `dbconcatKey` saves the realized (concatenated if
multiple) key columns corresponding to the `tblData`. Accessor
functions are also available for these 2 slots: 

```{r}
dbnrows(obj)
dbconcatKey(obj)
```

- `indexes` slot
The `indexes` slots is an unnamed list saving the row and column
indexes respectively corresponding to the `tblData` slot, so that the
`SQLDataFrame` could possibly have duplicate rows or only a subset of
data records from the `tblData`, while the `tblData` slot doesn't need
to be changed. To be consistent, the slots of `dbnrows` and
`dbconcatKey` will also remain unchanged.

```{r}
obj@indexes
obj_sub <- obj[sample(5, 3, replace = TRUE), 2:3]
obj_sub
obj_sub@indexes
identical(tblData(obj), tblData(obj_sub))
```

With a `filter` function (which is similar to `[i, ]` subsetting),
only the `indexes` slot will be updated for the row index pointing to
the `tblData`.

```{r}
obj_filter <- obj %>% filter(division == "South Atlantic" & size == "medium")
obj_filter@indexes
identical(tblData(obj), tblData(obj_filter))
```

- methods

The `ROWNAMES,SQLDataFrame` method was defined to return the
(concatenated by `\b` if multiple) key column(s) value, so that the row
subsetting with character vector works for the `SQLDataFrame` objects.

```{r}
obj1 <- SQLDataFrame(dbname = dbfile, dbtable = "state",
                     dbkey = c("region", "population"))

ROWNAMES(obj1[1:10,])
obj1[c("South\b3615.0", "West\b365.0"), ]
```

## `VariantExperiment` container for lazy infrastructures

### Introduction
[VariantExperiment][] is a _Bioconductor_ package that directly
extends [SummarizedExperiment][] to lazily represent very large
sequencing data (with metadata) with backends of VCF or GDS formats.

The high-throughput genetic/genomic assay data are saved as
[GDSArray][] or [VCFArray][] objects. In addition, the metadata for
features and samples (`rowData` and `colData`) are saved as
[DelayedDataFrame][] objects. So [VariantExperiment][] has enabled the
on-disk representation of both assay data and metadata. It uses
significantly less memory than in-memory R alternatives. So it is a
lightweight container for very large genomic data resources
represented as a complete experiment.

The implementation of [SummarizedExperiment][] interface enables easy
and common manipulations for high-throughput genetic/genomic data with
familiar _R/Bioconductor_ paradigms, and supports smooth
interoperability with widely used bioinformatics tools that are
available on _R/Bioconductor_.

[VariantExperiment]: https://bioconductor.org/packages/VariantExperiment 
[SummarizedExperiment]: https://bioconductor.org/packages/SummarizedExperiment
[SingleCellExperiment]: https://bioconductor.org/packages/SingleCellExperiment
[SeqArray]: https://bioconductor.org/packages/SeqArray
[SeqVarTools]: https://bioconductor.org/packages/SeqVarTools

### `VariantExperiment` class
VariantExperiment class is defined to extend the SummarizedExperiment
class. The difference would be that the assay data are saved as
GDSArray or VCFArray, and the metadata are saved by default as
DelayedDataFrame (with option to save as ordinary DataFrame). There
are coercion methods defined for both VCF and GDS files into
VariantExperiment objects.

The coercion function of `makeSummarizedExperimentFromGDS` coerces GDS
files into VariantExperiment objects directly, with the assay data
saved as GDSArray, and the rowData / colData in DelayedDataFrame
by default (with the option of ordinary DataFrame object).

```{r, VE gds constructor, eval=FALSE}
gds <- SeqArray::seqExampleFileName("gds")
ve <- makeSummarizedExperimentFromGDS(gds)
ve
```
```{r, VE gds accessors, eval=FALSE}
assays(ve)[[1]]
rowData(ve)
colData(ve)
```

### Constructor

Arguments in `makeSummarizedExperimentfromGDS()` could be specified to
take only certain annotation columns for features and samples. All
available data entries for arguments values could be retrieved by the
`showAvailable()` function with the gds file name as input.

```{r, showAvailable, eval=FALSE}
showAvailable(gds)
```

Note that the 'infoColumns' from gds file will be saved as columns
inside the rowData, with the prefix of 'info_'. 'rowDataOnDisk' and
'colDataOnDisk' could be set as 'FALSE' to save all metadata in
ordinary DataFrame format.

```{r, makeSummarizedExperimentFromGDSArgs, eval=FALSE}
ve3 <- makeSummarizedExperimentFromGDS(gds,
                                       rowDataColumns = c("ID", "ALT", "REF"),
                                       infoColumns = c("AC", "AN", "DP"),
                                       rowDataOnDisk = TRUE,
                                       colDataOnDisk = FALSE)
rowData(ve3)  ## DelayedDataFrame object
colData(ve3)  ## DataFrame object
```

If the VariantExperiment object is based on GDS backend, then the function
`gdsfile()` will return the file path to the GDS file.

```{r, VE gdsfile, eval=FALSE}
gdsfile(ve)
```

The `makeSummarizedExperimentFromVCF()` function (by default) converts
the VCF file into a GDS file (internally using
`SeqArray::seqVCF2GDS`), and then construct a VariantExperiment object
with the GDS file as backend.

```{r, VE vcf constructor, eval=FALSE}
vcf <- SeqArray::seqExampleFileName("vcf")
ve <- makeSummarizedExperimentFromVCF(vcf)
ve
gdsfile(ve)
```

Assay data is in GDSArray format, and the feature-related metadata
are in DelayedDataFrame class (with column data in GDSArray format).

```{r, VE vcf accessors, eval=FALSE}
assay(ve, 1)
rowData(ve)
``` 

For VCF input, Users could also have the opportunity to save the
sample related annotation directly into the VariantExperiment object,
by providing the file path to the 'sample.info' argument, and then
retrieve by `colData()`.

```{r, VE vcf sample info, eval=FALSE}
sampleInfo <- system.file("extdata", "Example_sampleInfo.txt",
                          package="VariantExperiment")
ve <- makeSummarizedExperimentFromVCF(vcf, sample.info = sampleInfo)
colData(ve)
```

Most of the argument are same as the
`makeSummarizedExperimentFromGDS`, with additional argument of 'start'
and 'count' to specify the start position and number of variants to
read into VariantExperiment object.

```{r, makeSummarizedExperimentFromVCFArgs_startCount, eval=FALSE}
ve2 <- makeSummarizedExperimentFromVCF(vcf, start=101, count=1000)
dim(ve2)
```
For the above example, only 1000 variants are read into the
VariantExperiment object, starting from the position of 101. 

The support of VCFArray in the coercion method of
`makeSummarizedExperimentFromVCF(useVCFArray = TRUE)` is under work
now. This will read the variant call data and metadata from VCF file,
and construct into VCFArray for the assay slot and the
DelayedDataFrame for the rowData and colData slots, with columns being
VCFArray objects.

Since the VCFArray was written internally using [VariantAnnotation][],
we are also working to enable the statistical funtions and methods on
the VariantExperiment objects with VCF backends. 
	
### Basic methods

Consistent with SummarizedExperiment, The VariantExperiment supports
`[` subsetting with numeric, logical and character indexes. The '$'
subsetting could also be operated directly on `colData()` columns, for
easy sample extraction.

- `[` and `$` subsetting

```{r, VE subset, eval=FALSE}
ve[1:10, 1:5]
ve$family
ve[, as.logical(ve$family == "1328")]  ## convert GDSArray into logical vector.
ve[as.logical(rowData(ve)$REF == "T"),]
```

- Range-based operations

VariantExperiment supports all of the `findOverlaps()` methods and
associated functions.  This includes `subsetByOverlaps()`, which makes
it easy to subset a VariantExperiment object by an interval.

```{r, VE subset by overlap, eval=FALSE}
ve1 <- subsetByOverlaps(ve, GRanges("22:1-48958933"))
ve1
```

### Save / load `VariantExperiment` object

Note that the operations on VariantExperiment are delayed.  So after
subsetting by '[', '$' or Ranged-based operations, and you feel
satisfied with the data for downstream analysis, you need to save that
VariantExperiment object to synchronize the on-disk file (in GDS or
VCF format) that is associated with the subset of data (in-memory
representation) before any statistical analysis. Otherwise, an error
will be returned.

For example, after we subset the 've' by 'GRanges("22:1-48958933")',
and we want to calculate the hwe based on the 23 variants, an error
will be generated indicating that we need to sync the on-disk and
in-memory representations.

```{r VEsaveLoad, eval=FALSE}
hwe(ve1)
## Error in .saveGDSMaybe(gdsfile) : use
##   'saveVariantExperiment()' to synchronize on-disk and
##   in-memory representations
```

Use the function `saveVariantExperiment()` to synchronize the on-disk
and in-memory representation, and reload into the same _R_ session.

```{r, VE save, eval=FALSE}
a <- tempfile()
ve1 <- saveVariantExperiment(ve1, dir=a, replace=TRUE)
ve1
gdsfile(ve1)
```

You can also use `loadVariantExperiment()` function to explicitly
reload any synchronized VariantExperiment object. The gds file path
to the new VariantExperiment object will be the 'se.gds' in the
specified directory.

```{r, VE load, eval=FALSE}
ve2 <- loadVariantExperiment(dir=a)
gdsfile(ve2)
```

Now we are all set for any downstream analysis as needed. 

```{r, VE stats demo, eval=FALSE}
head(hwe(ve1))
```

### Statistical methods

With GDS backend, Many statistical functions and methods are defined
on VariantExperiment objects, most of which has their generic
defined in _Bioconductor_ package of [SeqArray][] and
[SeqVarTools][]. These functions could be called directly on
VariantExperiment objects as input, with additional arguments to
specify based on user's need. More details please refer to the
vignettes of [SeqArray][] and [SeqVarTools][].

Here is a list of the statistical functions with brief description:

statistical functions | Description
--------------------- | ------------
seqAlleleFreq         | Calculates the allele frequencies
seqAlleleCount        | Calculates the allele counts 
seqMissing            | Calculates the missing rate for variant/sample
seqNumAllele          | Calculates the number of alleles (for ref/alt allele)
hwe                   | Exact test for Hardy-Weinberg equilibrium on Single-Nucleotide Variants
inbreedCoeff          | Calculates the inbreeding coefficient by variant/sample
pca                   | Calculates the eigenvalues and eignevectors with Principal Component Analysis
titv                  | Calculate transition/transversion ratio overall or by sample
refDosage             | Calculate the dosage of reference allele (matrix with integers of 0/1/2)
altDosage             | Calculate the dosage of alternative allele (matrix with integers of 0/1/2)
countSingletons       | Count singleton variants for each sample
heterozygosity        | Calculate heterozygosity rate by sample or by variants
homozygosity          | Calculate homozygosity rate by sample or by variants
meanBySample          | Calculate the mean value of a variable by sample over all variants
isSNV                 | Flag a single nucleotide variant 
isVariant             | Locate which samples are variant for each site

Here are some examples in calculating the sample missing rate, hwe,
titv ratio and the count of singletons for each sample.

```{r, VE stats, eval=FALSE}
## sample missing rate
mr.samp <- seqMissing(ve, per.variant = FALSE)
head(mr.samp)

## hwe
hwe <- hwe(ve)
head(hwe)

## titv ratio by sample / overall
titv <- titv(ve, by.sample=TRUE)
head(titv)
titv(ve, by.sample=FALSE)

## countSingletons
countSingletons(ve)
```


